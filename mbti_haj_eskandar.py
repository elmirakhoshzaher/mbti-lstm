# -*- coding: utf-8 -*-
"""MBTI-haj-eskandar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136SlJ6Eh9OufqhuFfh-jRrs5mRIG4YUv
"""

import pandas as pd

df = pd.read_excel('MBTI.xlsx')
print(df.shape)
df = df.drop(columns=df.columns[[0, 1]])
# i removed NAN value data is starting from column 2

# this is just random output from dataset
df.iloc[:,5]

# plot random column
count_series = df.iloc[:,5].value_counts()
print(count_series)
import matplotlib.pyplot as plt

count_series.plot(kind='bar', color=['blue', 'orange'])
plt.title('Distribution of Answers')
plt.xlabel('Answer')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt
for i in range(20):
  count_series = df.iloc[:, i].value_counts()
  colors = ['green' if val == 'A' else 'blue' for val in count_series.index]

  plt.figure()
  count_series.plot(kind='bar', color=colors)
  plt.title(f'{df.columns[i]}')
  plt.xlabel('Answer')
  plt.ylabel('Frequency')
  plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

data = pd.read_excel('/content/mbti1.xlsx', header=None) # here we are loading cleaned csv

data.shape
X = data.iloc[:, :59].values
Y = data.iloc[:, 60].values

#  'A' to 1  'b' to 0
X_numerical = np.array([[1 if val == 'A' else 0 for val in row] for row in X])


label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)

num_classes = 16
Y_one_hot = to_categorical(Y_encoded, num_classes=num_classes)

# (samples, time steps, features)
X_numerical = X_numerical.reshape((X_numerical.shape[0], X_numerical.shape[1], 1))


X_train, X_test, Y_train, Y_test = train_test_split(X_numerical, Y_one_hot, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)  #  (samples, timesteps, features)
print("Y_train shape:", Y_train.shape)  # (samples, num_classes)


print("Number of classes:", num_classes)

import tensorflow as tf

def recall_m(y_true, y_pred):
    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))
    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
import matplotlib.pyplot as plt


model = Sequential()


model.add(Bidirectional(LSTM(units=64, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], 1))))
model.add(Dropout(0.3))
model.add(Bidirectional(LSTM(units=32, activation='tanh')))
model.add(Dropout(0.3))


model.add(Dense(units=16, activation='softmax'))

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

def scheduler(epoch, lr):
    """Learning rate  function"""
    if epoch < 10:
        return lr
    else:
        new_lr = lr * tf.math.exp(-0.1)
        return float(new_lr.numpy())

lr_scheduler = LearningRateScheduler(scheduler)


history = model.fit(
    X_train, Y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping, lr_scheduler],
    verbose=2
)


plt.figure(figsize=(12, 6))


plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model.summary()



test_loss, test_acc = model.evaluate(X_test, Y_test)
print(f'Test Accuracy: {test_acc}')

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np


test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)

print(f'Test Accuracy: {test_acc:.4f}')


predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(Y_test, axis=1)

#Randomly print true label vs model predict
print("\nSample Predictions vs True Labels:")
for i in range(5):
    print(f"Sample {i+1}:")
    print(f"  True Label: {true_classes[i]}")
    print(f"  Predicted Label: {predicted_classes[i]}")
    print()

sample_index = 2 #randomly we are selecting 2
sample_data = X_test[sample_index]
sample_raw = data.iloc[sample_index, :X_test.shape[1]]


prediction = model.predict(np.expand_dims(sample_data, axis=0))
predicted_class = np.argmax(prediction, axis=1)[0]
predicted_label = label_encoder.inverse_transform([predicted_class])[0]


print(f"Raw Data (Row {sample_index}):")
print(sample_raw)
print(f"Predicted Class: {predicted_label}")







import pandas as pd
import numpy as np

file_path = 'mbti2.xlsx'
df = pd.read_excel(file_path, header=None)


df.columns = [f'Feature_{i+1}' for i in range(df.shape[1] - 1)] + ['Label']

df.replace({'A': 1, 'b': 0}, inplace=True)

num_synthetic_rows = 10000
num_features = df.shape[1] - 1


labels = df['Label'].unique()


synthetic_data = []

for _ in range(num_synthetic_rows): # becasue we have just. A and b just we need to generete 0 and 1
    features = np.random.choice([0, 1], size=num_features) # random 10000

    label = np.random.choice(labels)

    synthetic_data.append(np.append(features, label))

synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)

synthetic_df.to_excel('synthetic_data.xlsx', index=False)

print(synthetic_df)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical


data = pd.read_excel('/content/synthetic_data.xlsx')  # this is for data that we genereted it 10 000 raws


X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)


X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))


model = Sequential()
model.add(LSTM(100, input_shape=(1, X_scaled.shape[1]), return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(100))
model.add(Dropout(0.3))
model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)


history = model.fit(X_reshaped, y_categorical, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])


plt.figure(figsize=(12, 5))


plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])


plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'])

plt.show()


predictions = model.predict(X_reshaped)
predicted_classes = np.argmax(predictions, axis=1)
predicted_labels = label_encoder.inverse_transform(predicted_classes)

for i in range(min(10, len(y))):
    print(f"Original Label: {y[i]}, Predicted Label: {predicted_labels[i]}")

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import LearningRateScheduler
import matplotlib.pyplot as plt

data = pd.read_excel('/content/mbti1.xlsx', header=None)  # Adjust the filename as necessary
X = data.iloc[:, :59].values
Y = data.iloc[:, 60].values


X_numerical = np.array([[1 if val == 'A' else 0 for val in row] for row in X])


label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)
num_classes = 16
Y_one_hot = to_categorical(Y_encoded, num_classes=num_classes)


X_numerical = X_numerical.reshape((X_numerical.shape[0], X_numerical.shape[1], 1))

def create_model():
    model = Sequential()
    model.add(Bidirectional(LSTM(units=128, return_sequences=True, input_shape=(X_numerical.shape[1], 1),
                                 kernel_regularizer=tf.keras.regularizers.l2(0.01))))
    model.add(Dropout(0.3))
    model.add(Bidirectional(GRU(units=64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))))
    model.add(Dropout(0.3))
    model.add(Bidirectional(GRU(units=32, kernel_regularizer=tf.keras.regularizers.l2(0.01))))
    model.add(Dropout(0.3))
    model.add(Dense(units=num_classes, activation='softmax'))

    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model


def scheduler(epoch, lr):
    """Learning rate scheduler function"""
    if epoch < 20:
        return lr
    else:
        new_lr = lr * tf.math.exp(-0.05)
        return float(new_lr.numpy())

lr_scheduler = LearningRateScheduler(scheduler)


kf = KFold(n_splits=5, shuffle=True, random_state=42)

fold = 1
fold_results = []
for train_index, val_index in kf.split(X_numerical):
    X_train, X_val = X_numerical[train_index], X_numerical[val_index]
    Y_train, Y_val = Y_one_hot[train_index], Y_one_hot[val_index]

    model = create_model()

    history = model.fit(
        X_train, Y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, Y_val),
        callbacks=[lr_scheduler],
        verbose=2
    )

    val_loss, val_accuracy = model.evaluate(X_val, Y_val, verbose=0)
    fold_results.append((val_loss, val_accuracy))
    print(f"Fold {fold} - Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}")
    fold += 1


average_val_loss = np.mean([result[0] for result in fold_results])
average_val_accuracy = np.mean([result[1] for result in fold_results])
print(f"Average Validation Loss: {average_val_loss}")
print(f"Average Validation Accuracy: {average_val_accuracy}")


X_train_full, X_test, Y_train_full, Y_test = train_test_split(X_numerical, Y_one_hot, test_size=0.2, random_state=42)

final_model = create_model()
final_model.fit(
    X_train_full, Y_train_full,
    epochs=100,
    batch_size=32,
    verbose=2
)


test_loss, test_accuracy = final_model.evaluate(X_test, Y_test, verbose=0)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")


plt.figure(figsize=(12, 6))


plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt


data = pd.read_excel('/content/mbti1.xlsx', header=None)
X = data.iloc[:, :59].values
Y = data.iloc[:, 60].values

X_numerical = np.array([[1 if val == 'A' else 0 for val in row] for row in X])


label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)
num_classes = 16
Y_one_hot = to_categorical(Y_encoded, num_classes=num_classes)


X_train, X_test, Y_train, Y_test = train_test_split(X_numerical, Y_one_hot, test_size=0.2, random_state=42)


model = Sequential()
model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))


model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

#  overfitting in this data because of 0 1 over fitting is happning
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)


history = model.fit(
    X_train, Y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.3,
    callbacks=[early_stopping],
    verbose=2
)




plt.figure(figsize=(12, 6))


plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt


data = pd.read_excel('/content/mbti1.xlsx', header=None)
X = data.iloc[:, :59].values
Y = data.iloc[:, 60].values


X_numerical = np.array([[1 if val == 'A' else 0 for val in row] for row in X])


scaler = StandardScaler()
X_normalized = scaler.fit_transform(X_numerical)


label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)
num_classes = 16
Y_one_hot = to_categorical(Y_encoded, num_classes=num_classes)


X_reshaped = X_normalized.reshape((X_normalized.shape[0], 1, X_normalized.shape[1]))


X_train, X_test, Y_train, Y_test = train_test_split(X_reshaped, Y_one_hot, test_size=0.2, random_state=42)


model = Sequential()
model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(Dropout(0.5))
model.add(LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))


model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)


early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)


history = model.fit(
    X_train, Y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=2
)




plt.figure(figsize=(12, 6))


plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model.summary()

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt


data = pd.read_excel('/content/mbti1.xlsx', header=None)  # Adjust the filename as necessary
X = data.iloc[:, :59].values
Y = data.iloc[:, 60].values


X_numerical = np.array([[1 if val == 'A' else 0 for val in row] for row in X])


scaler = StandardScaler()
X_normalized = scaler.fit_transform(X_numerical)


label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)
num_classes = 16
Y_one_hot = to_categorical(Y_encoded, num_classes=num_classes)


X_reshaped = X_normalized.reshape((X_normalized.shape[0], 1, X_normalized.shape[1]))


X_train, X_test, Y_train, Y_test = train_test_split(X_reshaped, Y_one_hot, test_size=0.2, random_state=42)


model = Sequential()
model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(LSTM(32, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))


model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)


early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)


history = model.fit(
    X_train, Y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=2
)



plt.figure(figsize=(12, 6))


plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model.summary()

import numpy as np


num_samples = 10
X_sample = X_test[:num_samples]
Y_sample_true = Y_test[:num_samples]


predictions = model.predict(X_sample)


predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(Y_sample_true, axis=1)


predicted_labels = label_encoder.inverse_transform(predicted_classes)
true_labels = label_encoder.inverse_transform(true_classes)

for i in range(num_samples):
    print(f"Sample {i+1}:")
    print(f"True Label: {true_labels[i]}")
    print(f"Predicted Label: {predicted_labels[i]}")
    print()

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras.utils import to_categorical
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression


data = pd.read_excel('/content/mbti1.xlsx', header=None)  # Adjust the filename as necessary
X = data.iloc[:, :59].values
Y = data.iloc[:, 60].values


X_numerical = np.array([[1 if val == 'A' else 0 for val in row] for row in X])


scaler = StandardScaler()
X_normalized = scaler.fit_transform(X_numerical)


label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)
num_classes = len(np.unique(Y_encoded))
Y_one_hot = to_categorical(Y_encoded, num_classes=num_classes)


X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y_encoded, test_size=0.2, random_state=42)

Y_train_2d = Y_train
Y_test_2d = Y_test


class_names = label_encoder.classes_

def print_classification_report(model_name, Y_true, Y_pred):
    """
    Print classification report for a given model.
    """
    print(f"{model_name} Accuracy: {accuracy_score(Y_true, Y_pred)}")
    print(classification_report(Y_true, Y_pred, target_names=class_names))




decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, Y_train_2d)
Y_pred_tree = decision_tree.predict(X_test)
print_classification_report("Decision Tree", Y_test_2d, Y_pred_tree)




gbm = GradientBoostingClassifier(n_estimators=100)
gbm.fit(X_train, Y_train_2d)
Y_pred_gbm = gbm.predict(X_test)
print_classification_report("Gradient Boosting Machine", Y_test_2d, Y_pred_gbm)


adaboost = AdaBoostClassifier(n_estimators=100)
adaboost.fit(X_train, Y_train_2d)
Y_pred_adaboost = adaboost.predict(X_test)
print_classification_report("AdaBoost", Y_test_2d, Y_pred_adaboost)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import RidgeClassifier, Perceptron
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier


ridge_classifier = RidgeClassifier()
ridge_classifier.fit(X_train, Y_train_2d)
Y_pred_ridge = ridge_classifier.predict(X_test)
print_classification_report("Ridge Classifier", Y_test_2d, Y_pred_ridge)

perceptron = Perceptron()
perceptron.fit(X_train, Y_train_2d)
Y_pred_perceptron = perceptron.predict(X_test)
print_classification_report("Perceptron", Y_test_2d, Y_pred_perceptron)

qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, Y_train_2d)
Y_pred_qda = qda.predict(X_test)
print_classification_report("QDA", Y_test_2d, Y_pred_qda)